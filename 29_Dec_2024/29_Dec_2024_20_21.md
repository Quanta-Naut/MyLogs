# Tarun's Workspace File Summary
## Generated On: Sunday, December 29, 2024 at 8:21:13 PM
This summary lists all files in the workspace with brief descriptions.
---
### Project Description
This project appears to be related to machine learning, specifically focusing on classification algorithms. The presence of various model files (`.pkl`), a Jupyter Notebook for data preprocessing, and CSV files for training and testing data suggests that the project involves training and evaluating different machine learning models on a dataset. The project is likely educational, aimed at learning and comparing different machine learning techniques.

### File Descriptions

#### WorkspaceSummary.md
- **Purpose**: This file likely contains a summary of the workspace, including details about the project structure, files, and possibly the results of the machine learning models.
- **Type**: Documentation
- **Usage**: Provides an overview of the project and its components.

#### data_preprossecing.ipynb
- **Purpose**: This Jupyter Notebook is used for data preprocessing. It likely includes steps such as data cleaning, normalization, and feature engineering.
- **Type**: Code
- **Usage**: Prepares the raw data for training machine learning models.

#### decision_tree.pkl
- **Purpose**: This file contains a serialized Decision Tree model.
- **Type**: Model
- **Usage**: Used to save and load the trained Decision Tree model.

#### random_forest.pkl
- **Purpose**: This file contains a serialized Random Forest model.
- **Type**: Model
- **Usage**: Used to save and load the trained Random Forest model.

#### support_vector_machine.pkl
- **Purpose**: This file contains a serialized Support Vector Machine (SVM) model.
- **Type**: Model
- **Usage**: Used to save and load the trained SVM model.

#### neural_network_(mlp).pkl
- **Purpose**: This file contains a serialized Multi-Layer Perceptron (MLP) neural network model.
- **Type**: Model
- **Usage**: Used to save and load the trained MLP model.

#### naive_bayes.pkl
- **Purpose**: This file contains a serialized Naive Bayes model.
- **Type**: Model
- **Usage**: Used to save and load the trained Naive Bayes model.

#### logistic_regression.pkl
- **Purpose**: This file contains a serialized Logistic Regression model.
- **Type**: Model
- **Usage**: Used to save and load the trained Logistic Regression model.

#### k-nearest_neighbors.pkl
- **Purpose**: This file contains a serialized K-Nearest Neighbors (KNN) model.
- **Type**: Model
- **Usage**: Used to save and load the trained KNN model.

#### gradient_boosting.pkl
- **Purpose**: This file contains a serialized Gradient Boosting model.
- **Type**: Model
- **Usage**: Used to save and load the trained Gradient Boosting model.

#### test.csv
- **Purpose**: This CSV file contains the test dataset used to evaluate the performance of the machine learning models.
- **Type**: Data
- **Usage**: Used for model evaluation.

#### train.csv
- **Purpose**: This CSV file contains the training dataset used to train the machine learning models.
- **Type**: Data
- **Usage**: Used for model training.

#### gender_submission.csv
- **Purpose**: This CSV file likely contains a sample submission file, possibly for a Kaggle competition, indicating the format and structure of the expected output.
- **Type**: Data
- **Usage**: Used as a reference for creating submission files.

### Learning or Building
Based on the file names and their purposes, this project is primarily educational, aimed at learning and comparing different machine learning algorithms. The presence of multiple model files and a data preprocessing notebook suggests that the user is experimenting with various techniques to understand their performance on a given dataset. 
### Project Description:
 The provided code is a machine learning pipeline for predicting survival on the Titanic dataset. It includes data preprocessing, model training, evaluation, and comparison of multiple models. Here are some statistics about the code:

- **Number of lines**: 169
- **Number of functions**: 2 (`load_models_from_folder`, `get_titanic_input`)
- **Number of classes**: 0
- **Number of imported libraries**: 13 (`pandas`, `numpy`, `matplotlib.pyplot`, `seaborn`, `sklearn.model_selection`, `sklearn.linear_model`, `sklearn.metrics`, `sklearn.preprocessing`, `joblib`, `os`)
- **Number of models trained**: 8 (SVM, KNN, Decision Tree, Logistic Regression, Random Forest, Gradient Boosting, MLP, Naive Bayes)

The code performs the following tasks:
1. Imports necessary libraries.
2. Loads and preprocesses the Titanic dataset.
3. Converts categorical variables to numerical.
4. Scales the features.
5. Trains a logistic regression model and evaluates its performance.
6. Compares the performance of multiple models.
7. Saves the trained models.
8. Provides a function to load models and predict survival based on user input.
