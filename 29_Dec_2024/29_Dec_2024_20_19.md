# tarun's Workspace File Summary
## Generated On: Sunday, December 29, 2024 at 8:19:31 PM
This summary lists all files in the workspace with brief descriptions.
---
### Project Description
This project appears to be a machine learning project focused on classification algorithms. The goal is likely to compare the performance of various machine learning models on a given dataset. The project involves data preprocessing, model training, and evaluation.

### Relevant Files

#### WorkspaceSummary.md
- **Description**: This file likely contains a summary of the workspace, including the structure of the project, the purpose of each file, and possibly some notes on the progress and next steps.
- **Purpose**: Documentation
- **Project Type**: Building a project

#### data_preprossecing.ipynb
- **Description**: This Jupyter Notebook file is used for data preprocessing. It likely contains code for cleaning, transforming, and preparing the data for model training.
- **Purpose**: Data preprocessing
- **Project Type**: Building a project

#### support_vector_machine.pkl
- **Description**: This file contains a trained Support Vector Machine (SVM) model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### random_forest.pkl
- **Description**: This file contains a trained Random Forest model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### neural_network_(mlp).pkl
- **Description**: This file contains a trained Multi-Layer Perceptron (MLP) neural network model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### naive_bayes.pkl
- **Description**: This file contains a trained Naive Bayes model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### logistic_regression.pkl
- **Description**: This file contains a trained Logistic Regression model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### k-nearest_neighbors.pkl
- **Description**: This file contains a trained K-Nearest Neighbors (KNN) model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### gradient_boosting.pkl
- **Description**: This file contains a trained Gradient Boosting model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### decision_tree.pkl
- **Description**: This file contains a trained Decision Tree model serialized using Python's pickle module.
- **Purpose**: Model storage
- **Project Type**: Building a project

#### train.csv
- **Description**: This CSV file contains the training dataset used to train the machine learning models.
- **Purpose**: Training data
- **Project Type**: Building a project

#### test.csv
- **Description**: This CSV file contains the test dataset used to evaluate the performance of the trained machine learning models.
- **Purpose**: Testing data
- **Project Type**: Building a project

#### gender_submission.csv
- **Description**: This CSV file likely contains sample submission data, possibly for a Kaggle competition, indicating the format and structure of the expected output.
- **Purpose**: Sample submission
- **Project Type**: Building a project 
### Project Description:
 The provided code is a machine learning pipeline for predicting survival on the Titanic dataset. It includes data preprocessing, model training, evaluation, and comparison of multiple models. Here are some statistics about the code:

- **Number of lines**: 157
- **Number of functions**: 2 (`load_models_from_folder`, `get_titanic_input`)
- **Number of classes**: 0
- **Number of imported libraries**: 13
- **Number of models trained**: 8 (Support Vector Machine, K-Nearest Neighbors, Decision Tree, Logistic Regression, Random Forest, Gradient Boosting, Neural Network, Naive Bayes)

The code performs the following tasks:
1. Imports necessary libraries.
2. Loads and preprocesses the Titanic dataset.
3. Converts categorical variables to numerical.
4. Scales the features.
5. Trains a logistic regression model and evaluates its performance.
6. Trains and compares multiple models.
7. Saves the trained models.
8. Provides a function to load models and take user input for predictions.
