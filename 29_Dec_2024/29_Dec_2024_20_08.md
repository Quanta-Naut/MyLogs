# Workspace File Summary
## Generated On: Sunday, December 29, 2024 at 8:08:55 PM
This summary lists all files in the workspace with brief descriptions.
---
### Project Description
This project appears to be a machine learning project focused on classification algorithms. The goal is likely to compare the performance of various machine learning models on a given dataset. The project involves data preprocessing, model training, and evaluation.

### File Descriptions

#### WorkspaceSummary.md
- **Purpose**: This file likely contains a summary of the workspace, including details about the project structure, files, and possibly the steps taken during the project.
- **Project Role**: Documentation
- **Learning or Building**: Building a project

#### data_preprossecing.ipynb
- **Purpose**: This Jupyter Notebook file is used for data preprocessing. It likely includes steps such as data cleaning, normalization, and feature engineering.
- **Project Role**: Data Preparation
- **Learning or Building**: Building a project

#### decision_tree.pkl
- **Purpose**: This file contains a serialized Decision Tree model. It is used to save the trained model so it can be loaded and used later without retraining.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### k-nearest_neighbors.pkl
- **Purpose**: This file contains a serialized K-Nearest Neighbors model.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### gradient_boosting.pkl
- **Purpose**: This file contains a serialized Gradient Boosting model.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### support_vector_machine.pkl
- **Purpose**: This file contains a serialized Support Vector Machine model.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### random_forest.pkl
- **Purpose**: This file contains a serialized Random Forest model.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### neural_network_(mlp).pkl
- **Purpose**: This file contains a serialized Multi-Layer Perceptron (MLP) neural network model.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### naive_bayes.pkl
- **Purpose**: This file contains a serialized Naive Bayes model.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### logistic_regression.pkl
- **Purpose**: This file contains a serialized Logistic Regression model.
- **Project Role**: Model Storage
- **Learning or Building**: Building a project

#### train.csv
- **Purpose**: This CSV file contains the training dataset used to train the machine learning models.
- **Project Role**: Data
- **Learning or Building**: Building a project

#### test.csv
- **Purpose**: This CSV file contains the test dataset used to evaluate the performance of the trained models.
- **Project Role**: Data
- **Learning or Building**: Building a project

#### gender_submission.csv
- **Purpose**: This CSV file likely contains a sample submission file, possibly for a Kaggle competition, indicating the format for submitting predictions.
- **Project Role**: Data
- **Learning or Building**: Building a project 
### Project Description:
 The provided code is a machine learning pipeline for the Titanic dataset. It includes data preprocessing, model training, evaluation, and comparison of multiple classifiers. Here are the key statistics about the code:

- **Number of lines**: 155
- **Number of functions**: 2 (`load_models_from_folder`, `get_titanic_input`)
- **Number of classes**: 0
- **Number of imported libraries**: 12 (`pandas`, `numpy`, `matplotlib.pyplot`, `seaborn`, `sklearn.model_selection`, `sklearn.linear_model`, `sklearn.metrics`, `sklearn.preprocessing`, `joblib`, `os`, `sklearn.svm`, `sklearn.neighbors`, `sklearn.tree`, `sklearn.ensemble`, `sklearn.neural_network`, `sklearn.naive_bayes`)

The code performs the following tasks:
1. Imports necessary libraries.
2. Loads and preprocesses the Titanic dataset.
3. Converts categorical variables to numerical.
4. Scales the features.
5. Trains a logistic regression model and evaluates its performance.
6. Compares multiple machine learning models.
7. Saves the trained models.
8. Provides functions to load models and take user input for predictions.
9. Displays results and visualizations.
